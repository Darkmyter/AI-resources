# Notes and tips from papers

## [Attention Is All You Need](https://arxiv.org/abs/1706.03762), **Attention, 2017**
* Articles:
    * [How Attention works in Deep Learning: understanding the attention mechanism in sequence models](https://theaisummer.com/attention/)
    * [Yannic Kilcher video](https://www.youtube.com/watch?v=iDulhoQ2pro)
    * [DeepMind x UCL | Deep Learning Lectures | 8/12 | Attention and Memory in Deep Learning](https://www.youtube.com/watch?v=AIiwuClvH6k)
    * ‚≠ê [How Transformers work in deep learning and NLP: an intuitive introduction](https://theaisummer.com/transformer/)
